----------------------------------------------------------------------
PREDICT: 

dens0 -> dens1 -> loss
pred = w2*tanh(.0470*w1) = .0047
w1 = .4587
w2 = .2190
----------------------------------------------------------------------

-- layer_list[0], layer (input_layer0) --
wght, prev connection
   0.4587
prod, prev layer
[field column 0]
   0.0470


wght*prod
[field column 0]
   0.0216


incrInputs: x.n_rows= 1
inputs.n_rows= 1
-- nlayer, layer (dense01) --
nlayer
[field column 0]
   0.0216


wght, prev connection
   0.2190
prod, prev layer
[field column 0]
   0.0216


wght*prod
[field column 0]
   0.0047


incrInputs: x.n_rows= 1
inputs.n_rows= 1
-- nlayer, layer (dense12) --
nlayer
[field column 0]
   0.0047


return value
[field column 0]
   0.0047

Output: 0.004720667755633531  (CORRECT)
----------------------------------------------------------------------
input -> dens0 -> dens1
==> dLdw_exact
  -0.0634

----------------------------------------------------------------------
Created branch weight_recursive
----------------------------------------------------------------------
Check the sequences: prediction and back prop. 

1) dimension = 1, identity activation functions
   seq=2

 l=0    l=1    l=2
  In --> d1 --> d2 --> loss0    (t=0)
         |      |
         |      |
         v      v
  In --> d1 --> d2 --> loss1    (t=1)


Inputs to nodes: z(l,t), a(l,t-1)
Output to nodes: a(l,t)
Weights: In -- d1 : w1
Weights: d1 -- d2 : w12
Weights: d1 -- d1 : w11
Weights: d2 -- d2 : w22
d1: l=1
d2: l=2
exact(t): exact results at time t
loss0(a(2,0), exact(0))
loss1(a(2,1), exact(1))
Input at t=0: x0
Input at t=1: x1

Loss = L = loss0 + loss1
Forward: 
 a(1,-1) = 0, z(1,-1) = w11 * a(1,-1)
 a(2,-1) = 0, z(1,-1) = w22 * a(2,-1)
 a(1,0) = z(1,0) = w1*x0     + w12 * z(1,-1)
 a(2,0) = z(2,0) = w2*z(1,0) + w12 * z(2,-1)
 -------
 z(1,0) = w11 * a(1,0)
 z(2,0) = w21 * a(2,0)
 a(1,0) = z(2,0) = w1*x1     + w12 * z(1,0)
 a(2,0) = z(2,1) = w2*z(1,1) + w12 * z(2,0)
-----------
*** the model should have a variable called global loss, which the sum of individual loss that are collected. 

One takes the derivative of the sum of losses over the sequences with respect to the weights. Is that correct? 
----------------------------------------------------------------------
Sept. 2, 2016
test_recurrent5.cpp: prediction works, but only accurate to 4 digits compared to analytical in single precision. 
Indicates that double precision might be necessary on larger networks unless algorithms are stable. 
----------------------------------------------------------------------
Sept. 2, 2016
The gradient of the objective function appears to have the wrong sign. 
Fixed in objective.cpp, method gradient in MSE subclass. 
----------------------------------------------------------------------
Sept. 2, 2016, 4 pm
Backpropagation works with recurrent nets!
Next: biases. Then GMM! Then LSTM!

One bias per node. Consider a single layer. 
----------------------------------------------------------------------
a20, a21= 0.088806, 0.249670
Prediction: pred
[field column 0]
   0.0890   0.2552 

a20, a21= 0.088806, 0.249670
Prediction: pred
[field column 0]
   0.0900   0.2670

Input layer: 3
Output layer: 5
Weight: 5 x 3
Number biases: 5
Add another column to w
Now Weight: 5 x 4
So add unit component to the "a" layer output and 1 column to the weights:  
  z* = w* * a*   (w* has additional column, a* has a unit component). z* has the 
  same number of rows as before. 

The disadvantage of storing weights the way we do, via pointers (if biases are incorporated into the weights), 
is that one MUST share weights+biases at the same time. One cannot just share the weights and keep the biases individual. 
That is a RESTRICTION OF OUR APPROACH. 

Better though: the bias is a property of the layer. If a layer L1 has 128 nodes, 
and two layers L2 and L3, of size 512 impinge on L1, there are are still only 128 biases in L1, 
512 biases in L2 and another 512 biases in L3. 
So we'll store the biases with the layers. 

I will want to consider the biases to be on or off. Not clear how to do this. 
----------------------------------------------------------------------
Bias derivatives work for test_recurrent_model_bias2.cpp (2 recurrent nodes). 
----------------------------------------------------------------------
My guess: there is still a small error somewhere in the 3rd or 4th significant digit. Do not know where. 
So, TODO: 
- remove all print statements. 
- remove unused methods
- keep only the most general routines for recursion + weight updates with bias. 
----------------------------------------------------------------------
Sept. 4, 2016
- implemented ReLU
----------------------------------------------------------------------
Sept. 4, 2016
At this time, the Connection class has "WEIGHT weight" member variable, making it difficult to share weights between pointers. 
To do so (share weights), it would be be better if this member were a WEIGHT* weight; I must now change all locations in the code
using WEIGHT. 
If the I did not have a special typedef for weights, it would much hard to identify all the locations where change was required in the code. 
This little tidbit is for Nathan who complained one day about all the different typedefs to remember. 
--------------------------------------------------------------------------------------------------------------------------------------------
Sept. 4, 2016
Storing of data in Armadillo is columnwise. So I am storing weight as w(seq, dim), so matrix products w*x are efficient. However, 
w(2,5), follows w(1,5) in memory, like Fortran. Therefore, w(dim, seq) would be more efficient, and the matrix product should be x*w
instead of w*x, with x a row vector. TODO IN THE FUTURE. softmax would be cheaper as well since one must sum over dimensions to 
compute the denominator. However, ignore until total cost of program is diagnosed. 

VF2D_F variables are stored as x[batch](dimension, seq). So summing over dimensions is equivalent to summing over one column, as it should
be for efficiency. 

Now consider the derivatives. 
a = f(z) where a and z have dimension "layer_size". For scalar activation functions, one computes the gradient da/dz = f'
But for vector functions (such as the softmax), da/dz = J = jacobian of the transformation, which is a (layer_size X layer_size) matrix. 
Thus, the deltas dL/da(l) (l is the layer), is written as wght * (J * dL/da(l))
----------------------------------------------------------------------
Sep. 4, 2016
Consider: 
dL/da(l) = dL/da(l+1) * da(l+1)/dz(l+1) * dz(l+1)/da(l+1)
dL/da(l)(k) = dL/da(l+1)(p) * da(l+1)(p)/dz(l+1)(q) * dz(l+1)(q)/da(l)(k)
Given that: 
z(l+1)(q) = w(q,j) * a(l)(j), 
dz(l+1)(q)/da(l)(k) = w(q,j) * delta(k,j)  (Kronecker delta)
                      w(q,k) (Kronecker delta) (ignore layer indices)
We also have: 
da(l+1)(p)/dz(l+1)(q) = J(l+1)(p,q) = Jacobian of activation function 

Collecting and combining the above relations: 
dL/da(l)(k) = dL/da(l+1)(p) * J(l+1)(p,q) * w(q,k)
Need Jacobian of activation function
----------------------------------------------------------------------
Sept. 5, 2016
Question: Consider a connection from a layer to itself. should the layers contain the connection information, 
or should be considered a connection like any other connection? 
- Any other connection:  more consistent with the rest of the program
- Keeping the connection with the layer: the layer, if recurrent contains this connection and can treat it polymorphically. 
  However, the general code becomes more complex. What is the difference between a connection going from layers L1 to L1 with a 
  delay of 2 and a onnection going from layer L1 to layer L2 with a connection of 2? Shouldn't their treatment be identical? 
- Also, how do LSTM's work? They have internal connections that are a part of the LSTM (they form in a sense a "complex" nodes. 
  In fact, these complex nodes can be thought of as a model in themselves. either the single LSTM could be a Model, or a layer of 
  LSTMs could be a model (probably better, especially if the weihgts of different LSTMs within a layer are connected (as they are, in 
  Keras, for example.)
----------------------------------------------------------------------
Sept. 5, 2016
Demonstrate making a change in common.cpp and recompiling as an example of why implementations should not be included in a file. 
Rather they should be in a class. Compilation takes too long. if only the .h file is included, long compilations only occur when the .h
file is changed, which happens less often than changes in the implementation cpp file. 
----------------------------------------------------------------------
Sept. 6, 2016
Check Softmax and derivatives. Create a network with input, a single layer of 4 elements. Check that Softmax works. Use mse objective function.
----------------------------------------------------------------------
Sept. 9, 2016
To handle arbitrary delays, put all this in the connections. Add two variables: 
t_from = 0
t_to = delay
t_ : initially set to t_from, incremented every iteration. When t_ == t_to, 
send data from input to the output via the weights. More generally, one could solve
differential equations. 
SEEMS RATHER ELEGANT. 
----------------------------------------------------------------------
<<<<<<< HEAD
Oct. 3, 2016
Add timings with Bollig's timing library. 
=======
Sept. 9, 2016
Bias: something wrong with batch > 1. I doubt it is in the testing. 
Weight deriv. Works with nb_batch > 1 (there was summation error in tests/
-
All tests codes EXCEPT test_recurrent_model_bias5.cpp, have backprop working with nb_batch > 1. 
SO ONE ERROR TO FIX. 
----------------------------------------------------------------------
Sept. 10, 2016
tests/test_recurrent_model_bias1/test_recurrent_model_bias1 -l 4  -b 1 -s 2 -a iden -r 1 -w "xavier"
   (high derivatives of weights)
tests/test_recurrent_model_bias1/test_recurrent_model_bias1 -l 4  -b 1 -s 3 -a iden -r 1 -w "xavier"
   (low derivatives of weights)
SOMETHING WRONG. 
----------------------------------------------------------------------
Need matrix with *Bias1.cpp for large largers and large sequences. 
Calculate eigenvalues. Use identity activation. is_recursion = 1. 
>>>>>>> weight_class
----------------------------------------------------------------------
Sept. 15, 2016
Branch "double" in git. 
Work in double precision. 
----------------------------------------------------------------------
Sept. 16, 2016
Construct a copy of bias1 code. Remove print statements. Add extra variables in layers.h, Connections and perhaps model.h
in order to help with error diagnosis. 
----------------------------------------------------------------------
INSIDE derivloss
alpha= -0.023967
m0, n0= 1, 1
w11= -0.023967
e= 0.750000
z0= 0.295810
k= 0


 derivLoss, l.. (loss)
 -0.9083803553
 -1.2013459311
 dl = 0.000000
 INSIDE derivloss
 alpha= -0.023967
 m0, n0= 1, 1
 w11= -0.023967
 e= 0.750000
 z0= 0.295810
 k= 1
--------------------------------------------------------------------------
The model should not have an input_dim. Only the input layers should. 

I doubt this results is a function of the layer size. 

DOUBLE PRECISION
tests/debug_bias1/debug_bias1 -l 4  -b 1 -s 364 -a iden -r 1 -w xavier_iden -i .001
diagonal element: 1.02
abs_err_norm:  4.9263580e-06
rel_err_norm:  5.8023473e-15
max abs error:  2.5033951e-06
max rel error:  2.9423055e-15
  at weight: 850827721.9395312

DOUBLE PRECISION
tests/debug_bias1/debug_bias1 -l 4  -b 1 -s 364 -a iden -r 1 -w xavier_iden -i .001
diagonal element: 0.98
abs_err_norm:  2.7175785e-12
rel_err_norm:  2.7044032e-15
max abs error:  1.1368684e-12
max rel error:  1.4606659e-15
  at weight_bp:    778.3219815

SINGLE PRECISION
tests/debug_bias1/debug_bias1 -l 4  -b 1 -s 364 -a iden -r 1 -w xavier_iden -i .001
abs_err_norm:  3.2653477e+03
rel_err_norm:  2.4677461e-06
max abs error: -6.0800000e+02
max rel error:  1.2906820e-06
  at weight_bp: -471068800.0000000

SINGLE PRECISION
tests/debug_bias1/debug_bias1 -l 4  -b 1 -s 364 -a iden -r 1 -w xavier_iden -i .001
abs_err_norm:  1.4477477e-03
rel_err_norm:  1.4488168e-06
max abs error:  2.4414062e-04
max rel error:  3.1367503e-07
  at weight_bp:    778.3234253

Next: keep the diagonal at 0.98, and use finite-differences in single and double precision. 
FINITE-DIFFERENCES VERY IMPRECISE
DOUBLE PRECISION: 

tests/debug_bias1/debug_bias1 -l 4  -b 1 -s 12 -a iden -r 1 -w xavier_iden -i 1.e-8
Connection (weight2), weight(4, 1), layers: (input_layer0, rdense1), type: spatial
weight: w,abs,rel=  1.7026641e+01,  5.5363446e-13,  6.3667533e-14
max rel error:  4.4592673e-14 at weight_bp: -9.719782

tests/debug_bias1/debug_bias1 -l 4  -b 1 -s 12 -a iden -r 1 -w xavier_iden -i 1.e-6
Connection (loop_conn0), weight(4, 4), layers: (rdense1, rdense1), type: temporal
weight: w,abs,rel=  7.4725563e+01,  2.5106331e+01,  2.1394613e+00
max rel error:  7.9091690e-01 at weight_bp: -4.524829

FINITE-difference very inaccurate with seq_len12 and inc=1.e-6!!! 

SINGLE PRECISION

tests/debug_bias1/debug_bias1 -l 4  -b 1 -s 12 -a iden -r 1 -w xavier_iden -i 1.e-6





delta=1.e-3
tests/debug_bias1/debug_bias1 -l 4  -b 1 -s 364 -a iden -r 1 -w xavier_iden -i .001
----------------------------------------------------------------------
Oct. 25, 2016
- 1) run tests for recursive networks, make sure everything works. Keep networks very simple. 
- 2) Add parameters + gradients to activation functions. 
----------------------------------------------------------------------
Oct. 27, 2016
Weights are initialized as w(input, output), but are stored as w(output, input). 
That is very confusing. At some point, I should change this so that weight 
are initialized as w(output, input). 

So if I define w1 = WEIGHT(3,5) and w = WEIGHT(5,7), 
the multiplication w1*w does not work, but w*w1 does. 

I am afraid that there are too many places to change in the code!
----------------------------------------------------------------------
Oct. 28, 2016
development branch
test_recurrent_model_bias1 : seems to run ok, with seq_len=2. However, 
when seq_len=3, accuracy of FD derivatives decreases for recursive links. Do not know whether my solution is correct or not. 
----------------------------------------------------------------------
Do not use arma::size(...)  Used as np.dim in python. It is not an integer!
----------------------------------------------------------------------
Nov. 5, 2016
Are armadillo fields consecutive in memory? It would greatly simplify things when creating input to the prediction routine. 
The the conversion of data is only done once. 
----------------------------------------------------------------------
Nov. 5, 2016
I wonder if I have an error. If a layer has 16 nodes, how many different biases are there? 16? 
----------------------------------------------------------------------
I must freeze biases as well. 
I just created is_bias_frozen with set and get operations in Layer
Freeze the biases. This probably accounts for the errors I am getting. 
----------------------------------------------------------------------
Found the error! I forget to reset the deltas to zero (for activation parameters) 
before starting a new batch.
----------------------------------------------------------------------
I don't think that rms error is best measure when function decays exponentially. 
Perhaps the log(rms error)? Sum of (y-ex)^2 is RMS error. Is the arg minimum of log(y-ex)^2 the 
same as the arg min(rms error)
----------------------------------------------------------------------
Nov. 12, 2016
I duplicate results from Mark's code: exp(-alpha*dt), alpha=1, starting with alpha=2. 
600 steps in t, dt=0.005, 200 epochs. Unfreezing biases leads to instability. 
Unfreezing weights leads to convergence to a different state. 

Must experiment with speeding up convergence, and with more complex networks. 
How to impose constraints between weights (like the sum of two weights = 1?)
development git branch. 

Create a git tag: v.02
----------------------------------------------------------------------
Try with seq_len > 1
Try with batch_size > 1
----------------------------------------------------------------------
Nov. 13, 2015
test_diff_eq4: set certain connections to temporal manually to make it easier to 
create clist() for the spatial connections. clist is normally constructed by connectionOrderCleanOrig. Contrary to what I thought, this routine cannot handle all cases. So let us ignore connections already identified as temporal. 

Perhaps I need to keep the temporal links in a separate list from the spatial links, 
and include in this list all the temporal links from the RecurrentLayers. This might 
actually simplify things. 

Of course, this manual approach will make it hard to handle reservoir networks, unless all the connections are treated as temporal. 

Create a new branch called temporal to deal with this problem. 
----------------------------------------------------------------------
Nov. 13, branch temporal. 
Perhaps do not update "next" and "prev" when the connection is temporal. That would imply specifying the link
is temporal at the time I execute that "add" command. 
----------------------------------------------------------------------
Nov. 14, 2016, branch temporal
added loop_input, and forwardLoops to Layer.cpp. This will serve as input to temporal connections. 
In the future, allow for multiple temporal inputs into a layer. 
----------------------------------------------------------------------
Nov. 16, 2016, branch temporal
Apparently, prediction is different from cfe3fe8. 

FOUND THE ERROR: When doing FD tests, I am calling predict multiple times. I must do this with stateful = false. 
Fix this problem in model.cpp: resetState() at the top of the prediction method. 
----------------------------------------------------------------------
Nov. 17, 2016, branch temporal
When seq_len=2, there is a difference between the two code: 
layer, loop input
0 -0.19281525099  (for code that works)
0  0              (for code that does not work)

Figured the error out: recurrent layers have a temporal connection. In the new code, 
the temporal connections are contained in Model::clist_temporal. So I must change initialize_weights
to initialize the recucrrent connections correctly. 

I need a way to initialize weight using a preestablished sequence of random or non-random numbers. Otherwise
it becomes hard to compare multiple codes with each other. Alternatively, how to I initiate a random 
stream? 

PROBLEM: number of times Connection::initialize() is called is different for the two codes. DO NOT UNDERSTAND WHY. 
----------------------------------------------------------------------
Nov. 17, 2016, branch temporal
seq_len=4, diff_eq3, I get nan with lr=2 (works with lr=10, but wrong parameter value)
ANOTHER BUG? Yes. Initialization of xf() was incorrect. 

seq_len=1, alpha goes from 1 to 2. Weird.  Learning_rate = 10. 
seq_len=2, alpha goes from 1 to 3. Weird.  Learning_rate = 10. 
seq_len=4, alpha goes from 1 to 5. Weird.  Learning_rate = 10. 
seq_len=4, alpha goes from 1 to 5. Weird.  Learning_rate = 20. 
seq_len=6, alpha goes from 1 to 7. Weird.  Learning_rate = 20. 
----------------------------------------------------------------------
Something changed: even with seq_len=1, I get alpha going to 4.455 . What did I do? 
----------------------------------------------------------------------
seq_len=4 (x(0)=0.5 ==> alpha->5, x(0)=1 ==> alpha => 6.64...
For some reason, I must start with with 
	for (int e=0; e < nb_epochs; e++) {
		in[0][0,0] = 0.5 * net_inputs[0][0][0,0]; // <<<<< NOTE: 0.5 coefficient. WHY?  Weight is already 1/2
		Reason is that I am multiplying by the fixed weight of 1/2 manually. (as opposed to the code doing it. I 
		am effectively imposing state from a fictitious previous step. I should probably multiply by the weight instead
		of 1/2 in case the weight changes one day.)
		printf("**** Epoch %d ****\n", e);
----------------------------------------------------------------------
November 18
Learn Rate: 20. 
nb_epochs: 400
nb_points (samples): 600
seq_len=5: 36 epochs needed for alpha to go from 1.0 to 1.99
seq_len=10: 18 epochs needed for alpha to go from 1.0 to 1.99
seq_len=20: 9 epochs needed for alpha to go from 1.0 to 1.99
seq_len=40: 2 epochs needed for alpha to go from 1.0 to 1.99
----------------------------------------------------------------------
Nov. 18, 2016
Trying layer=2. There is a problem with matrix incompatibility. 
----------------------------------------------------------------------
Nov. 21, 2016
diff_eq5: with Tanh on the 2nd node. 
seq_len = 5 and 10: the alpha goes to 2.04 and 1.857, respectively. Why isn't the 
convergence to the same value? But why should it be since the network is different, and 
does not match the discretization of a differential equation. 

When I freeze only the two weights that have constraints on them, the weight from first to second
node converges to 66, which is rather large. Do not know why. I need to visualize the intermediate inputs. 
Note that the tanh squashes the solution, removing the effect of the large weihgt. so in Neural 
networks, what prevents the weights from growing too large? 
----------------------------------------------------------------------
Nov. 22, 2016
Added parameter history (in Model.h) and weight histories (in Connection.h)
----------------------------------------------------------------------
Nov. 22, 2016
test_diff_eq3 no longer working properly. Do not know why. 
----------------------------------------------------------------------
Nov. 23, 2016
I do get convergence for the two-node case, but the loss function is not decreasing. 

The loss function goes down during an epoch, but then goes up before decreasing again (when there are two nodes
in the network: DecayDE and Tanh). So ultimately, I believe that after reaching some minimum, the loss function
increases and the system goes into some other steady state, that is not consistent with the desired solution of 
the differential equation. To confirm this, I must run the prediction routine (with seq_len=1 and stateful=True). 
----------------------------------------------------------------------
Nov. 23, 2016
Using log(MSE) as an objective function failed miserably. Perhaps lr too high? 
----------------------------------------------------------------------
Nov. 24, 2016
m->setSeqLen(5): sets model sequence length. But sequence length is also defined in the layers. If 
seq_len is defined, all the arrays must be redimensioned the way we do things. 

test_diff_eq4.cpp: two DecayDE nodes. 
seq_len affects results. seq_len=60 slows down convergence. Do not know why. 
----------------------------------------------------------------------
Nov. 24, 2016
- 2 nodes in parallel. (test_diff_eq6.cpp). Works. I get the same alph for both nodes. That makes sense. 
So I should try this with two different kinds of equations. 

I tried a function to fit: exp(alpha1*x) + exp(alpha2*x), and got no success in figuring out the equation. 
Perhaps my equations are too simple? 

I cannot get any kind of convergence. Rather bothersome. S
	ExpFunc ==> REAL operator()(REAL x) { return exp(-alpha*x); }
	Func& fun1 = *(new ExpFunc(2.));
	Func& fun2 = *(new ExpFunc(-.3));
	fun = fun1 + fun2;
----------------------------------------------------------------------
Nov. 26, 2016
- Test batch mode. Batch size of some layers get reset to 1. 
- Problem: the gradient of the objective function somehow has the incorrect batch size. Check this. 
----------------------------------------------------------------------
Dec. 1, 2016
- WeightedMeanSquareError not working. Some kind of col error. Why? 
- MeanSquareError : try pure relative error

Must find a way to choose relative vs absolute error for objective function. Relative error runs much faster. 
Add new variable in objective.h and in model.h: error_type (abs, rel)
----------------------------------------------------------------------
Dec. 2, 2016
Problem: I am not convinced that predict works properly. I will have to do this by hand. Why? Because different initial 
values of the parameters seem to give the same solution predictions (with seq_len=1), and simply running trainOne over and 
over again. That is not reasonable. 
----------------------------------------------------------------------
Dec. 3, 2016
Implement char-rnn and Gaussian Mixture Model. 
Then I can go back to work on predicting signals. 
----------------------------------------------------------------------
Dec. 4, 2016
- Never tested coupled derivatives of Jacobian activation function. 
- Never tested back propagation with softmax (I should create simple network with 2 inputs and 2 outputs)
  Input(2) --> softmax(2) --> Least squares or cross-entropy
  Input(2) --> softmax(3) --> Least squares or cross-entropy
  Input(3) --> softmax(3) --> Least squares or cross-entropy
----------------------------------------------------------------------
Memory leaks
Use valgrind
==25294== 80 (64 direct, 16 indirect) bytes in 1 blocks are definitely lost in loss record 67 of 236
==25294== 104 (56 direct, 48 indirect) bytes in 1 blocks are definitely lost in loss record 68 of 236
==25294== 176 bytes in 1 blocks are definitely lost in loss record 99 of 236
==25294== 176 bytes in 1 blocks are definitely lost in loss record 100 of 236
==25294== 176 bytes in 1 blocks are definitely lost in loss record 101 of 236
==25294== 176 bytes in 1 blocks are definitely lost in loss record 102 of 236
==25294== 232 (64 direct, 168 indirect) bytes in 1 blocks are definitely lost in loss record 108 of 236
==25294== 272 bytes in 1 blocks are definitely lost in loss record 115 of 236
==25294== 272 bytes in 1 blocks are definitely lost in loss record 116 of 236
==25294== 272 bytes in 1 blocks are definitely lost in loss record 117 of 236
==25294== 608 bytes in 1 blocks are definitely lost in loss record 138 of 236
==25294== 2,064 (1,600 direct, 464 indirect) bytes in 25 blocks are definitely lost in loss record 165 of 236
==25294== 2,800 (1,600 direct, 1,200 indirect) bytes in 25 blocks are definitely lost in loss record 169 of 236
==25294== 4,096 bytes in 1 blocks are definitely lost in loss record 173 of 236
==25294== 316,224 (4,224 direct, 312,000 indirect) bytes in 24 blocks are definitely lost in loss record 215 of 236
==25294== 316,224 (4,224 direct, 312,000 indirect) bytes in 24 blocks are definitely lost in loss record 216 of 236
==25294== 316,224 (4,224 direct, 312,000 indirect) bytes in 24 blocks are definitely lost in loss record 217 of 236
==25294== 316,224 (4,224 direct, 312,000 indirect) bytes in 24 blocks are definitely lost in loss record 218 of 236
==25294== 504,400 (4,400 direct, 500,000 indirect) bytes in 25 blocks are definitely lost in loss record 224 of 236
==25294== 504,400 (4,400 direct, 500,000 indirect) bytes in 25 blocks are definitely lost in loss record 225 of 236
==25294== 504,400 (4,400 direct, 500,000 indirect) bytes in 25 blocks are definitely lost in loss record 226 of 236
==25294== 504,400 (4,400 direct, 500,000 indirect) bytes in 25 blocks are definitely lost in loss record 227 of 236
==25294== 25,236,576 (544 direct, 25,236,032 indirect) bytes in 1 blocks are definitely lost in loss record 236 of 236
==25294==    definitely lost: 44,648 bytes in 259 blocks

==25390== 80 (64 direct, 16 indirect) bytes in 1 blocks are definitely lost in loss record 60 of 176
==25390== 104 (56 direct, 48 indirect) bytes in 1 blocks are definitely lost in loss record 61 of 176
==25390== 176 bytes in 1 blocks are definitely lost in loss record 74 of 176
==25390== 176 bytes in 1 blocks are definitely lost in loss record 75 of 176
==25390== 176 bytes in 1 blocks are definitely lost in loss record 76 of 176
==25390== 176 bytes in 1 blocks are definitely lost in loss record 77 of 176
==25390== 232 (64 direct, 168 indirect) bytes in 1 blocks are definitely lost in loss record 83 of 176
==25390== 272 bytes in 1 blocks are definitely lost in loss record 87 of 176
==25390== 272 bytes in 1 blocks are definitely lost in loss record 88 of 176
==25390== 272 bytes in 1 blocks are definitely lost in loss record 89 of 176
==25390== 608 bytes in 1 blocks are definitely lost in loss record 102 of 176
==25390== 2,064 (1,600 direct, 464 indirect) bytes in 25 blocks are definitely lost in loss record 126 of 176
==25390== 2,800 (1,600 direct, 1,200 indirect) bytes in 25 blocks are definitely lost in loss record 127 of 176
==25390== 4,096 bytes in 1 blocks are definitely lost in loss record 130 of 176
==25390== 316,224 (4,224 direct, 312,000 indirect) bytes in 24 blocks are definitely lost in loss record 158 of 176
==25390== 316,224 (4,224 direct, 312,000 indirect) bytes in 24 blocks are definitely lost in loss record 159 of 176
==25390== 316,224 (4,224 direct, 312,000 indirect) bytes in 24 blocks are definitely lost in loss record 160 of 176
==25390== 316,224 (4,224 direct, 312,000 indirect) bytes in 24 blocks are definitely lost in loss record 161 of 176
==25390== 504,400 (4,400 direct, 500,000 indirect) bytes in 25 blocks are definitely lost in loss record 166 of 176
==25390== 504,400 (4,400 direct, 500,000 indirect) bytes in 25 blocks are definitely lost in loss record 167 of 176
==25390== 504,400 (4,400 direct, 500,000 indirect) bytes in 25 blocks are definitely lost in loss record 168 of 176
==25390== 504,400 (4,400 direct, 500,000 indirect) bytes in 25 blocks are definitely lost in loss record 169 of 176
==25390== 19,559,152 (544 direct, 19,558,608 indirect) bytes in 1 blocks are definitely lost in loss record 176 of 176
==25390==    definitely lost: 44,648 bytes in 259 blocks
----------------------------------------------------------------------
Dec. 5, 2016
Clip gradient of cross-entropy to [-5,5]. Same as char-rnn. 
Perhaps I need ReLU instead of Tanh? 

I have a problem starting char-rnn. It blocks. 
----------------------------------------------------------------------
Dec. 6, 2016
Loss function appears incorrect in char-rnn. 
----------------------------------------------------------------------
Dec. 17, 2016
I cannot share activations between layers because they are deleted by the layer destructor. 
Better solution is to use auto_ptrs or something like that. 

I am running category classification with 2 classes. I find that the loss function first decreases from 9 to 0.001 in a few 
epochs, then returns back to its original value and oscillations. The lr is constant = 0.01. Still, kind of strange. 

A lr of 0.1 makes the loss oscillate. A loss of 0.01 makes loss go from 2.26 to 0.40 and then stays at 0.4 .  A loss of 0.001 makes loss
go towards 4.04 monotonically. Batch = 400

Batch = 32: loss (lr=0.001) eventually oscillates between 1.04 and 0.43 . So minimum is different than with larger batch. 
----------------------------------------------------------------------
