----------------------------------------------------------------------
PREDICT: 

dens0 -> dens1 -> loss
pred = w2*tanh(.0470*w1) = .0047
w1 = .4587
w2 = .2190
----------------------------------------------------------------------

-- layer_list[0], layer (input_layer0) --
wght, prev connection
   0.4587
prod, prev layer
[field column 0]
   0.0470


wght*prod
[field column 0]
   0.0216


incrInputs: x.n_rows= 1
inputs.n_rows= 1
-- nlayer, layer (dense01) --
nlayer
[field column 0]
   0.0216


wght, prev connection
   0.2190
prod, prev layer
[field column 0]
   0.0216


wght*prod
[field column 0]
   0.0047


incrInputs: x.n_rows= 1
inputs.n_rows= 1
-- nlayer, layer (dense12) --
nlayer
[field column 0]
   0.0047


return value
[field column 0]
   0.0047

Output: 0.004720667755633531  (CORRECT)
----------------------------------------------------------------------
input -> dens0 -> dens1
==> dLdw_exact
  -0.0634

----------------------------------------------------------------------
Created branch weight_recursive
----------------------------------------------------------------------
Check the sequences: prediction and back prop. 

1) dimension = 1, identity activation functions
   seq=2

 l=0    l=1    l=2
  In --> d1 --> d2 --> loss0    (t=0)
         |      |
         |      |
         v      v
  In --> d1 --> d2 --> loss1    (t=1)


Inputs to nodes: z(l,t), a(l,t-1)
Output to nodes: a(l,t)
Weights: In -- d1 : w1
Weights: d1 -- d2 : w12
Weights: d1 -- d1 : w11
Weights: d2 -- d2 : w22
d1: l=1
d2: l=2
exact(t): exact results at time t
loss0(a(2,0), exact(0))
loss1(a(2,1), exact(1))
Input at t=0: x0
Input at t=1: x1

Loss = L = loss0 + loss1
Forward: 
 a(1,-1) = 0, z(1,-1) = w11 * a(1,-1)
 a(2,-1) = 0, z(1,-1) = w22 * a(2,-1)
 a(1,0) = z(1,0) = w1*x0     + w12 * z(1,-1)
 a(2,0) = z(2,0) = w2*z(1,0) + w12 * z(2,-1)
 -------
 z(1,0) = w11 * a(1,0)
 z(2,0) = w21 * a(2,0)
 a(1,0) = z(2,0) = w1*x1     + w12 * z(1,0)
 a(2,0) = z(2,1) = w2*z(1,1) + w12 * z(2,0)
-----------
*** the model should have a variable called global loss, which the sum of individual loss that are collected. 

One takes the derivative of the sum of losses over the sequences with respect to the weights. Is that correct? 
----------------------------------------------------------------------
Sept. 2, 2016
test_recurrent5.cpp: prediction works, but only accurate to 4 digits compared to analytical in single precision. 
Indicates that double precision might be necessary on larger networks unless algorithms are stable. 
----------------------------------------------------------------------
Sept. 2, 2016
The gradient of the objective function appears to have the wrong sign. 
Fixed in objective.cpp, method gradient in MSE subclass. 
----------------------------------------------------------------------
Sept. 2, 2016, 4 pm
Backpropagation works with recurrent nets!
Next: biases. Then GMM! Then LSTM!

One bias per node. Consider a single layer. 
----------------------------------------------------------------------
a20, a21= 0.088806, 0.249670
Prediction: pred
[field column 0]
   0.0890   0.2552 

a20, a21= 0.088806, 0.249670
Prediction: pred
[field column 0]
   0.0900   0.2670

Input layer: 3
Output layer: 5
Weight: 5 x 3
Number biases: 5
Add another column to w
Now Weight: 5 x 4
So add unit component to the "a" layer output and 1 column to the weights:  
  z* = w* * a*   (w* has additional column, a* has a unit component). z* has the 
  same number of rows as before. 

The disadvantage of storing weights the way we do, via pointers (if biases are incorporated into the weights), 
is that one MUST share weights+biases at the same time. One cannot just share the weights and keep the biases individual. 
That is a RESTRICTION OF OUR APPROACH. 

Better though: the bias is a property of the layer. If a layer L1 has 128 nodes, 
and two layers L2 and L3, of size 512 impinge on L1, there are are still only 128 biases in L1, 
512 biases in L2 and another 512 biases in L3. 
So we'll store the biases with the layers. 

I will want to consider the biases to be on or off. Not clear how to do this. 
----------------------------------------------------------------------
Bias derivatives work for test_recurrent_model_bias2.cpp (2 recurrent nodes). 
----------------------------------------------------------------------
