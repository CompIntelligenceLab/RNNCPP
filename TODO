- How to incorporate frozen weights? 
- Create a library to link against (Cmakefile): Nathan
- Weight initialization
- Create single layer feedforward and print out the outputs
- create a code to run all the tests at once and provide a method
to determine success or failure. 

- Need to implement a deep copy assignment operator for the Optimzer class. The
  Model class' copy constructor uses the following line...
    *optimizer = *m.optimizer;
- add to layer: pre-activation values. So postactivation is simply the application of one function
  This is the "inputs" member variable. 
- Create 
----------------------------------------------------------------------
Features: 
VF1D_F loss;  // different batches stored at different locations

Implemented a function to generate a sine wave. Returns VF2D (x,y)

User is currently responsible for shaping the function to use as required
for a particular application. Specialized routines might  be made available.
----------------------------------------------------------------------
Aug 12, 2016
- Create a Weight class for increased flexibility.  (done)
- Can create subclasses for specialized layers. 
- In the future, layer classes can be templatized by weight type. 
- is it possible to derive a activation function f(x) such that: 
     x(t) = f(x(t-tau)) ? if so, we can do delays with back propagation. 
	 Assume eps=tau << 1
	 x(t) = f(x(t) - eps xdot(t)) = f(x(t)) - eps dfdx(x(t)) * xdot(t)
	 x = f - eps f' xdot
----------------------------------------------------------------------
Aug. 12, 2016
- For now, we'll assume no shared weights. One weight matrix per connection. Different connections, different weight matrices. 
- Identity activation in input layers. 

- I should allow for multiple inputs into my model (with different dimensionalities)
  - in this case, I need a) an InputLayer class, and b) an additional loop in predictComplex
  - allow connections not only like:    h(t+1) =  f(h(t), x(t)) (recursion), but I would allow
    things such as:     h(t+1) = f(h(t), 

In predictComplex():
	Step 1) for each layer in a list (which only has the input layers to start), collect all the "next" layers. 
	Step 2) remove all the previous layers from the list (only the new layers remain)
	Step 3) for each "next" layer, collect all the spatial "prev" connections, compute w*x and call the activation function for that layer. 

	Return to step 1. 

