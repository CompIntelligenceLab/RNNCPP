- How to incorporate frozen weights? 
- Create a library to link against (Cmakefile): Nathan
- Weight initialization
- Create single layer feedforward and print out the outputs
- create a code to run all the tests at once and provide a method
to determine success or failure. 

- Need to implement a deep copy assignment operator for the Optimzer class. The
  Model class' copy constructor uses the following line...
    *optimizer = *m.optimizer;
- add to layer: pre-activation values. So postactivation is simply the application of one function
  This is the "inputs" member variable. 
- Create 
----------------------------------------------------------------------
Features: 
VF1D_F loss;  // different batches stored at different locations

Implemented a function to generate a sine wave. Returns VF2D (x,y)

User is currently responsible for shaping the function to use as required
for a particular application. Specialized routines might  be made available.
----------------------------------------------------------------------
Aug 12, 2016
- Create a Weight class for increased flexibility.  (done)
- Can create subclasses for specialized layers. 
- In the future, layer classes can be templatized by weight type. 
- is it possible to derive a activation function f(x) such that: 
     x(t) = f(x(t-tau)) ? if so, we can do delays with back propagation. 
	 Assume eps=tau << 1
	 x(t) = f(x(t) - eps xdot(t)) = f(x(t)) - eps dfdx(x(t)) * xdot(t)
	 x = f - eps f' xdot
----------------------------------------------------------------------
Aug. 12, 2016
- For now, we'll assume no shared weights. One weight matrix per connection. Different connections, different weight matrices. 
- Identity activation in input layers. 

- I should allow for multiple inputs into my model (with different dimensionalities)
  - in this case, I need a) an InputLayer class, and b) an additional loop in predictComplex
  - allow connections not only like:    h(t+1) =  f(h(t), x(t)) (recursion), but I would allow
    things such as:     h(t+1) = f(h(t), 

In predictComplex():
	Step 1) for each layer in a list (which only has the input layers to start), collect all the "next" layers. 
	Step 2) remove all the previous layers from the list (only the new layers remain)
	Step 3) for each "next" layer, collect all the spatial "prev" connections, compute w*x and call the activation function for that layer. 

	Return to step 1. 
----------------------------------------------------------------------
Aug. 16, 2016
- Probably better to have user specify which links are temporal, and even specify the order in which information flows
through the system, at least for now. 
----------------------------------------------------------------------
Aug. 24, 2016
To treat temporal connections, separate out connection scans for spatial from temporal scans. 
// there should always be data (or zero) at the input node of a temporal connection

Perhaps I need a connection from the output to the loss. In that way, when scanning the connections, 
the last connection will go from the output_layer, which can then forward information to the loss function, 
and to backward nodes via recursion. 

Create a loss layer, which cannot have recursion. It is only there to make prediction and backprop more consistent across different types of networks. 
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
----------------------------------------------------------------------
